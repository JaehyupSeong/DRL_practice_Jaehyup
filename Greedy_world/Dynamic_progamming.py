import numpy as np
from Greed_world.environment import Env, GraphicDisplay


class PolicyIteration:
    # TODO: Please recall all the below explanation before starting to implement a code (Policy iteration '23. 01. 03!!)
    """
    1) To call the class "PolicyIteration", input "env" is required.
    2) When class "PolicyIteration" is called, self parameters in "def __init__" will be automatically generated.
    3) By using self, its attributes will uniquely generated, when object (specific instance) is generated.
    4) All the methods(def) requires "self" to be called, otherwise, it will occur an error
    5) While calling a methods(def) in the same class, "self" is required, since 4)
    6) __init__ should be always generated by starting with "self", and second input is a variable of "class"
    7) Tensor is generated as column -> row -> depth
    8) Call from the most outer part to most inner part ([0][1][2])
    9) The axis = 0 (depth), axis = 1 (row), and axis = 2 (column) print depth, row, and column, respectively
    """

    def __init__(self, env):
        self.env = env
        # Initialize the value function of each state
        self.valuable_table = [[0.0]*env.width for _ in range(env.height)]  # env.width = 5, env.height = 5
        # Uniformly initialize the policy of each state
        self.policy_table = [[[0.25, 0.25, 0.25, 0.25]] * env.width for _ in range(env.height)]
        self.policy_table[2][2] = []
        self.discount_factor = 0.9

    def policy_evaluation(self):
        next_value_table = [[0.00]*self.env.width for _ in range(self.env.height)]

        for state in self.env.get_all_states():
            # All the states are updated sequentially in an each loop
            value = 0.0

            # Value function of terminal point is zero
            # To skip the rest of the current iteration of a loop and move on to the next step
            if state == [2, 2]:
                next_value_table[state[0]][state[1]] = value
                continue

            for action in self.env.possible_actions:  # This "for" loop is for possible actions in each state
                # [(-1, 0), (1, 0), (0, -1), (0, 1)] -> Action index: 1,2,3,4
                next_state = self.env.state_after_action(state, action)
                reward = self.env.get_reward(state, action)  # Zero value w/o specific points
                next_value = self.get_value(next_state)  # Value functions are initialized as 0
                value += (self.get_policy(state)[action] * (reward + self.discount_factor * next_value))  # Value of state is accumulated

            next_value_table[state[0]][state[1]] = value  # After calculating value function, it is stored

        self.valuable_table = next_value_table  # Value table is updated after each policy evaluation

    def policy_improvement(self):
        next_policy = self.policy_table
        for state in self.env.get_all_states():

            # Policy does not have to be updated in terminal state
            # To skip the rest of the current iteration of a loop and move on to the next step
            if state == [2, 2]:
                continue

            value_list = []
            result = [0.0, 0.0, 0.0, 0.0]

            # Calculate value function occurred by each action in each state
            for index, action in enumerate(self.env.possible_actions):  # 0,1,2,3
                next_state = self.env.state_after_action(state, action)
                reward = self.env.get_reward(state, action)
                next_value = self.get_value(next_state)
                value = reward + self.discount_factor * next_value
                value_list.append(value)

            max_idx_list = np.argwhere(value_list == np.amax(value_list))
            # -> Shape is column vector
            # Not exactly same as np.argmax(value_list); It can extract all the index that value is np.amx(value_list)
            max_idx_list = max_idx_list.flatten().tolist()
            prob = 1 / len(max_idx_list)

            for idx in max_idx_list:
                result[idx] = prob
            # Only maximum value functions will be selected

            next_policy[state[0]][state[1]] = result

        self.policy_table = next_policy



    def get_policy(self, state):
        return self.policy_table[state[0]][state[1]]

    def get_value(self, state):
        return self.valuable_table[state[0]][state[1]]

env = Env()
a = PolicyIteration(env)
a.policy_improvement()